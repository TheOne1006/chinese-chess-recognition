{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../\"\n",
    "# 数据集的目录\n",
    "dataset_dir_train = os.path.join(root_dir, \"data/cchess_multi_label_layout\", \"train\")\n",
    "dataset_dir_val = os.path.join(root_dir, \"data/cchess_multi_label_layout\", \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历数据集\n",
    "def get_all_files(dataset_dir):\n",
    "    file_list = os.listdir(dataset_dir)\n",
    "\n",
    "    sorted_file_list = sorted(file_list)\n",
    "    \n",
    "    target_files = []\n",
    "\n",
    "    for file in sorted_file_list:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "            # ann\n",
    "            ann_file = file.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\")\n",
    "            if not os.path.exists(os.path.join(dataset_dir, ann_file)):\n",
    "                print(f\"ann_file not exists: {ann_file}\")\n",
    "                continue\n",
    "            target_files.append(os.path.join(dataset_dir, file))\n",
    "            \n",
    "    return target_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20661\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_files = get_all_files(dataset_dir_train)\n",
    "all_files.extend(get_all_files(dataset_dir_val))\n",
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除 fifityone, 重新创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset chess_multi_label not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m project_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchess_multi_label\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/core/dataset.py:233\u001b[0m, in \u001b[0;36mdelete_dataset\u001b[0;34m(name, verbose)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete_dataset\u001b[39m(name, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deletes the FiftyOne dataset with the given name.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m        name: the name of the dataset\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m        verbose (False): whether to log the name of the deleted dataset\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mdelete()\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/core/dataset.py:172\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(name, create_if_necessary)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(name, _create\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/core/dataset.py:167\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(name, create_if_necessary)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the FiftyOne dataset with the given name.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mTo create a new dataset, use the :class:`Dataset` constructor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    a :class:`Dataset`\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DatasetNotFoundError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m create_if_necessary:\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/core/singletons.py:36\u001b[0m, in \u001b[0;36mDatasetSingleton.__call__\u001b[0;34m(cls, name, _create, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     _create\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mdeleted\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     34\u001b[0m ):\n\u001b[1;32m     35\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     name \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39mname  \u001b[38;5;66;03m# `__init__` may have changed `name`\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/core/dataset.py:319\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, name, persistent, overwrite, _create, _virtual, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     doc, sample_doc_cls, frame_doc_cls \u001b[38;5;241m=\u001b[39m _create_dataset(\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m, name, persistent\u001b[38;5;241m=\u001b[39mpersistent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     doc, sample_doc_cls, frame_doc_cls \u001b[38;5;241m=\u001b[39m \u001b[43m_load_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_virtual\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc \u001b[38;5;241m=\u001b[39m doc\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_doc_cls \u001b[38;5;241m=\u001b[39m sample_doc_cls\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/core/dataset.py:8345\u001b[0m, in \u001b[0;36m_load_dataset\u001b[0;34m(obj, name, virtual)\u001b[0m\n\u001b[1;32m   8343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_dataset\u001b[39m(obj, name, virtual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   8344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m virtual:\n\u001b[0;32m-> 8345\u001b[0m         \u001b[43mfomi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmigrate_dataset_if_necessary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   8348\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _do_load_dataset(obj, name)\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/migrations/runner.py:228\u001b[0m, in \u001b[0;36mmigrate_dataset_if_necessary\u001b[0;34m(name, destination, error_level, verbose)\u001b[0m\n\u001b[1;32m    226\u001b[0m     _migrate_dataset_if_necessary(name, destination, verbose)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 228\u001b[0m     \u001b[43mfou\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_level\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/core/utils.py:681\u001b[0m, in \u001b[0;36mhandle_error\u001b[0;34m(error, error_level, base_error)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_error\u001b[39m(error, error_level, base_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    669\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Handles the error at the specified error level.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m        base_error: (optional) a base Exception from which to raise ``error``\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m     \u001b[43metau\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/eta/core/utils.py:1000\u001b[0m, in \u001b[0;36mhandle_error\u001b[0;34m(error, error_level, base_error)\u001b[0m\n\u001b[1;32m    998\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(error, base_error)\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1003\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(error)\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/migrations/runner.py:226\u001b[0m, in \u001b[0;36mmigrate_dataset_if_necessary\u001b[0;34m(name, destination, error_level, verbose)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Migrates the dataset from its current revision to the specified\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03mdestination revision.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    verbose (False): whether to log incremental migrations that are run\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     \u001b[43m_migrate_dataset_if_necessary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    228\u001b[0m     fou\u001b[38;5;241m.\u001b[39mhandle_error(e, error_level\u001b[38;5;241m=\u001b[39merror_level)\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/migrations/runner.py:235\u001b[0m, in \u001b[0;36m_migrate_dataset_if_necessary\u001b[0;34m(name, destination, verbose)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _migrations_disabled():\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_revision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m db_version \u001b[38;5;241m=\u001b[39m get_database_revision()\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/fiftyone/migrations/runner.py:62\u001b[0m, in \u001b[0;36mget_dataset_revision\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     60\u001b[0m dataset_doc \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mfind_one({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: name}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m})\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fo\u001b[38;5;241m.\u001b[39mDatasetNotFoundError(name)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset_doc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset chess_multi_label not found"
     ]
    }
   ],
   "source": [
    "project_name = \"chess_multi_label\"\n",
    "\n",
    "fo.delete_dataset(project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 22:29:20.752166476 [W:onnxruntime:, session_state.cc:1136 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2025-03-07 22:29:20.752178519 [W:onnxruntime:, session_state.cc:1138 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from onnx_classifier.full_classifier import FULL_CLASSIFIER_ONNX\n",
    "\n",
    "full_classifier = FULL_CLASSIFIER_ONNX(\n",
    "    model_path=os.path.join('..', \"work_dirs/deploy_0307/cchess_reg.onnx\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ann_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m     annotation \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 36\u001b[0m _, short_labels, confidence_10x9, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfull_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m annotation \u001b[38;5;241m=\u001b[39m annotation\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# annotation_arr_10_9 为 10 行 9 列的二维数组\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# ['.C....r..', '....a....', '...kba...', 'p.p.p.pCp', '..b.....c', '.cP.n.P..', 'P.......P', 'BR..Ba...', '....KR...', '...N.a.N.']\u001b[39;00m\n",
      "File \u001b[0;32m~/my-programes/chinese-chess-recognition/cchess_reg/examples/onnx_classifier/full_classifier.py:135\u001b[0m, in \u001b[0;36mFULL_CLASSIFIER_ONNX.pred\u001b[0;34m(self, image, is_rgb)\u001b[0m\n\u001b[1;32m    131\u001b[0m     img_bgr \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    133\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(img_bgr, is_rgb)\n\u001b[0;32m--> 135\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# 校验 labels 的 shape\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[0;32m~/my-programes/chinese-chess-recognition/cchess_reg/examples/onnx_classifier/full_classifier.py:113\u001b[0m, in \u001b[0;36mFULL_CLASSIFIER_ONNX.run_inference\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mRun inference on the image.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    tuple: A tuple containing the detection results and labels.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# 运行推理\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m outputs, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/ult-yolo/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:200\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    198\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all_files\n",
    "import tqdm\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "start_center_x = 50\n",
    "start_center_y = 50\n",
    "\n",
    "image_width = 450\n",
    "image_height = 500\n",
    "\n",
    "# 图片原尺寸 width = 450, height = 500\n",
    "\n",
    "item_width = (image_width - 50) / 9\n",
    "item_height = (image_height - 50) / 10\n",
    "\n",
    "\n",
    "batch_size = 32  # 设置批处理大小\n",
    "\n",
    "# 创建数据集\n",
    "dataset = fo.Dataset(\"chess_multi_label\")\n",
    "\n",
    "# 遍历所有图片文件\n",
    "for img_path in tqdm.tqdm(all_files):\n",
    "    # 获取对应的标注文件路径\n",
    "    ann_path = img_path.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\")\n",
    "\n",
    "    if not os.path.exists(ann_path):\n",
    "        print(f\"ann_path not exists: {ann_path}\")\n",
    "        continue\n",
    "    \n",
    "    annotation = ''\n",
    "    with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "        annotation = f.read()\n",
    "\n",
    "    _, short_labels, confidence_10x9, _ = full_classifier.pred(img_path)\n",
    "\n",
    "\n",
    "    annotation = annotation.strip()\n",
    "    # annotation_arr_10_9 为 10 行 9 列的二维数组\n",
    "    # ['.C....r..', '....a....', '...kba...', 'p.p.p.pCp', '..b.....c', '.cP.n.P..', 'P.......P', 'BR..Ba...', '....KR...', '...N.a.N.']\n",
    "    annotation_10_rows = [item for item in annotation.split(\"\\n\")]\n",
    "    # 将 annotation_10_rows 转换成为 10 行 9 列的二维数组\n",
    "    annotation_arr_10_9 = [list(item) for item in annotation_10_rows]\n",
    "\n",
    "\n",
    "    detections = []\n",
    "\n",
    "\n",
    "    for row_index, row in enumerate(annotation_arr_10_9):\n",
    "        for col_index, col in enumerate(row):\n",
    "            if col != '.':\n",
    "                detections.append(fo.Detection(\n",
    "                    label=col,\n",
    "                    bounding_box=[\n",
    "                        (start_center_x + col_index * item_width - item_width / 2) / image_width, \n",
    "                        (start_center_y + row_index * item_height - item_height / 2) / image_height, \n",
    "                        item_width / image_width, \n",
    "                        item_height / image_height\n",
    "                    ]\n",
    "                ))\n",
    "\n",
    "    pred_detections = []\n",
    "    for row_index, row in enumerate(short_labels):\n",
    "        for col_index, col in enumerate(row):\n",
    "            if col != '.':\n",
    "                pred_detections.append(fo.Detection(\n",
    "                    label=col,\n",
    "                    confidence=confidence_10x9[row_index][col_index],\n",
    "                    bounding_box=[\n",
    "                        (start_center_x + col_index * item_width - item_width / 2) / image_width, \n",
    "                        (start_center_y + row_index * item_height - item_height / 2) / image_height, \n",
    "                        item_width / image_width, \n",
    "                        item_height / image_height\n",
    "                    ]\n",
    "                ))\n",
    "\n",
    "    diff_detections = []\n",
    "    padding = 10\n",
    "    for row_index in range(10):\n",
    "        for col_index in range(9):\n",
    "            ann_label = annotation_arr_10_9[row_index][col_index]\n",
    "            pred_label = short_labels[row_index][col_index]\n",
    "\n",
    "            # 忽略 x 和 .\n",
    "            if ann_label == 'x' and pred_label == '.':\n",
    "                continue\n",
    "\n",
    "            if ann_label == '.' and pred_label == 'x':\n",
    "                continue\n",
    "\n",
    "            # 不一致才展示\n",
    "            if ann_label != pred_label:\n",
    "                diff_detections.append(fo.Detection(\n",
    "                    label=f\"{ann_label} -> {pred_label}\",\n",
    "                    bounding_box=[\n",
    "                        (start_center_x + col_index * item_width - item_width / 2 + padding) / image_width, \n",
    "                        (start_center_y + row_index * item_height - item_height / 2 + padding) / image_height, \n",
    "                        (item_width - padding * 2) / image_width, \n",
    "                        (item_height - padding * 2) / image_height\n",
    "                    ]\n",
    "                ))\n",
    "\n",
    "    base_name = os.path.basename(img_path)\n",
    "\n",
    "    tag=\"js_v2\"\n",
    "\n",
    "    # 判断 base_name 是否以 js_v2_ 开头\n",
    "    if not base_name.startswith(\"js_v2_\"):\n",
    "\n",
    "        if \"_\" in base_name:\n",
    "            base_name_arr = base_name.split(\"_\")\n",
    "        else:\n",
    "            base_name_arr = base_name.split(\"-\")\n",
    "        \n",
    "        tag = base_name_arr[0]\n",
    "    \n",
    "    sample = fo.Sample(filepath=img_path, tags=[tag])\n",
    "        # 添加多标签分类信息\n",
    "    sample['ground_truth'] = fo.Detections(\n",
    "        detections=detections\n",
    "    )\n",
    "    sample['predictions'] = fo.Detections(\n",
    "        detections=pred_detections\n",
    "    )\n",
    "    sample['diff'] = fo.Detections(\n",
    "        detections=diff_detections\n",
    "    )\n",
    "    # all_samples.append(sample)\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "\n",
    "\n",
    "# dataset.default_config = \n",
    "#     draw_config=draw_config\n",
    "# )\n",
    "\n",
    "# 保存数据集\n",
    "dataset.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建各种 view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找 文件以 'js' 开头\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# 修正: 含有 js_v2_ 的 文件, 且存在 diff 标签\n",
    "js_diff_view = dataset.match({\n",
    "    \"filepath\": {\"$regex\": \".*js_v2_.*\"},\n",
    "})\n",
    "\n",
    "js_diff_view = js_diff_view.match({\n",
    "    \"diff.detections\": {\"$not\": {\"$size\": 0}}  # 检测结果数组非空\n",
    "})\n",
    "# 保存 view\n",
    "js_diff_view.save(\"js_diff_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(js_diff_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diff_view = dataset.match({\n",
    "    \"diff.detections\": {\"$not\": {\"$size\": 0}}  # 检测结果数组非空\n",
    "})\n",
    "# 保存 view\n",
    "all_diff_view.save(\"all_diff_view\")\n",
    "dataset.save_view(\"all_diff_view\", all_diff_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_diff_view)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cchess_reg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
